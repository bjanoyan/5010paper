
@article{delgrange_simple_nodate,
	title = {Simple {Strategies} in {Multi}-{Objective} {MDPs}},
	abstract = {We consider the veriﬁcation of multiple expected reward objectives at once on Markov decision processes (MDPs). This enables a trade-oﬀ analysis among multiple objectives by obtaining a Pareto front. We focus on strategies that are easy to employ and implement. That is, strategies that are pure (no randomization) and have bounded memory. We show that checking whether a point is achievable by a pure stationary strategy is NP-complete, even for two objectives, and we provide an MILP encoding to solve the corresponding problem. The bounded memory case is treated by a product construction. Experimental results using Storm and Gurobi show the feasibility of our algorithms.},
	language = {en},
	author = {Delgrange, Florent and Katoen, Joost-Pieter and Quatmann, Tim and Randour, Mickael},
	pages = {26},
	file = {Delgrange et al. - Simple Strategies in Multi-Objective MDPs.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\TE9FTNT4\\Delgrange et al. - Simple Strategies in Multi-Objective MDPs.pdf:application/pdf},
}

@article{simm_reinforcement_nodate,
	title = {Reinforcement {Learning} for {Molecular} {Design}  {Guided} by {Quantum} {Mechanics}},
	abstract = {Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. Existing approaches work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MOLGYM, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efﬁciently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space.},
	language = {en},
	author = {Simm, Gregor N C and Pinsler, Robert and Hernández-Lobato, José Miguel},
	pages = {15},
	file = {Simm et al. - Reinforcement Learning for Molecular Design  Guide.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\QWS4TWUY\\Simm et al. - Reinforcement Learning for Molecular Design  Guide.pdf:application/pdf},
}

@article{ruggeri_technical_2016,
	title = {Technical {Note}: {Development} of chemoinformatic tools to enumerate functional groups in molecules for organic aerosol characterization},
	volume = {16},
	issn = {1680-7324},
	shorttitle = {Technical {Note}},
	url = {https://acp.copernicus.org/articles/16/4401/2016/},
	doi = {10.5194/acp-16-4401-2016},
	abstract = {Functional groups (FGs) can be used as a reduced representation of organic aerosol composition in both ambient and controlled chamber studies, as they retain a certain chemical speciﬁcity. Furthermore, FG composition has been informative for source apportionment, and various models based on a group contribution framework have been developed to calculate physicochemical properties of organic compounds. In this work, we provide a set of validated chemoinformatic patterns that correspond to (1) a complete set of functional groups that can entirely describe the molecules comprised in the α-pinene and 1,3,5trimethylbenzene MCMv3.2 oxidation schemes, (2) FGs that are measurable by Fourier transform infrared spectroscopy (FTIR), (3) groups incorporated in the SIMPOL.1 vapor pressure estimation model, and (4) bonds necessary for the calculation of carbon oxidation state. We also provide example applications for this set of patterns. We compare available aerosol composition reported by chemical speciation measurements and FTIR for different emission sources, and calculate the FG contribution to the O : C ratio of simulated gasphase composition generated from α-pinene photooxidation (using the MCMv3.2 oxidation scheme).},
	language = {en},
	number = {7},
	urldate = {2021-08-22},
	journal = {Atmospheric Chemistry and Physics},
	author = {Ruggeri, Giulia and Takahama, Satoshi},
	month = apr,
	year = {2016},
	pages = {4401--4422},
	file = {Ruggeri and Takahama - 2016 - Technical Note Development of chemoinformatic too.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\C3ZIIA5Z\\Ruggeri and Takahama - 2016 - Technical Note Development of chemoinformatic too.pdf:application/pdf},
}

@article{liu_modified_2018,
	title = {Modified {Structural} {Constraints} for {Candidate} {Molecule} {Generation} in {Computer}-{Aided} {Molecular} {Design}},
	volume = {57},
	issn = {0888-5885, 1520-5045},
	url = {https://pubs.acs.org/doi/10.1021/acs.iecr.7b04621},
	doi = {10.1021/acs.iecr.7b04621},
	abstract = {Computer-aided molecular design (CAMD) has attracted much attention in the past 30 years. The generation of a candidate molecular structure satisfying a set of structural constraints is an important part of such a problem. However, the commonly used structural constraints proposed by Odele and Macchietto [Odele, O.; Macchietto, S. Computer Aided Molecular Design: A Novel Method for Optimal Solvent Selection. Fluid Phase Equilib. 1993, 82, 47], cannot provide suﬃcient and necessary conditions for generating cyclic molecules. In this paper, the suﬃcient and necessary conditions for molecular generation were presented. According to these conditions, some modiﬁcations of the conventional constraints were made, and mathematical proofs demonstrated that the modiﬁed constraints can perfectly generate structurally feasible molecules with no more than two rings. Moreover, some new constraints were proposed to help generate feasible aromatic molecules with less than two rings. Finally, several cases were presented to validate the correctness and applicability of the proposed modiﬁcations via comparing the results of the CAMD problems using the constraints before and after modiﬁcation, and the new constraints were applied to design solvents for liquid−liquid extraction. It is shown that the modiﬁcations presented in this paper can improve the reliability of results of CAMD, and additionally, the conditions of molecular generation proposed in this paper can be easily used to derive structural constraints for any type of target molecules.},
	language = {en},
	number = {20},
	urldate = {2021-08-22},
	journal = {Industrial \& Engineering Chemistry Research},
	author = {Liu, Xinyu and Zhao, Yuehong and Ning, Pengge and Cao, Hongbin and Wen, Hao},
	month = may,
	year = {2018},
	pages = {6937--6946},
	file = {Liu et al. - 2018 - Modified Structural Constraints for Candidate Mole.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\KHD78W8H\\Liu et al. - 2018 - Modified Structural Constraints for Candidate Mole.pdf:application/pdf},
}

@book{achenie_computer_2003,
	address = {Amsterdam ; Boston},
	edition = {1st ed},
	series = {Computer-aided chemical engineering},
	title = {Computer aided molecular design: theory and practice},
	isbn = {978-0-444-51283-3},
	shorttitle = {Computer aided molecular design},
	language = {en},
	number = {12},
	publisher = {Elsevier},
	editor = {Achenie, Luke E. K. and Gani, R. and Venkatasubramanian, Venkat},
	year = {2003},
	keywords = {Data processing, Models Data processing, Molecular structure, Molecules},
	file = {Achenie et al. - 2003 - Computer aided molecular design theory and practi.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\B8XEPCJN\\Achenie et al. - 2003 - Computer aided molecular design theory and practi.pdf:application/pdf},
}

@article{hukkerikar_estimation_2012,
	title = {Estimation of {Environment}-{Related} {Properties} of {Chemicals} for {Design} of {Sustainable} {Processes}: {Development} of {Group}-{Contribution} $^{\textrm{+}}$ ({GC} $^{\textrm{+}}$ ) {Property} {Models} and {Uncertainty} {Analysis}},
	volume = {52},
	issn = {1549-9596, 1549-960X},
	shorttitle = {Estimation of {Environment}-{Related} {Properties} of {Chemicals} for {Design} of {Sustainable} {Processes}},
	url = {https://pubs.acs.org/doi/10.1021/ci300350r},
	doi = {10.1021/ci300350r},
	language = {en},
	number = {11},
	urldate = {2021-08-22},
	journal = {Journal of Chemical Information and Modeling},
	author = {Hukkerikar, Amol Shivajirao and Kalakul, Sawitree and Sarup, Bent and Young, Douglas M. and Sin, Gürkan and Gani, Rafiqul},
	month = nov,
	year = {2012},
	pages = {2823--2839},
	file = {Hukkerikar et al. - 2012 - Estimation of Environment-Related Properties of Ch.PDF:C\:\\Users\\Bryan\\Zotero\\storage\\GQU5UZGL\\Hukkerikar et al. - 2012 - Estimation of Environment-Related Properties of Ch.PDF:application/pdf},
}

@article{kang_estimation_2002,
	title = {Estimation of {Mixture} {Properties} from {First}- and {Second}-{Order} {Group} {Contributions} with the {UNIFAC} {Model}},
	volume = {41},
	issn = {0888-5885, 1520-5045},
	url = {https://pubs.acs.org/doi/10.1021/ie010861w},
	doi = {10.1021/ie010861w},
	language = {en},
	number = {13},
	urldate = {2021-08-22},
	journal = {Industrial \& Engineering Chemistry Research},
	author = {Kang, Jeong Won and Abildskov, Jens and Gani, Rafiqul and Cobas, José},
	month = jun,
	year = {2002},
	pages = {3260--3273},
	file = {Kang et al. - 2002 - Estimation of Mixture Properties from First- and S.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\UIIMGPGZ\\Kang et al. - 2002 - Estimation of Mixture Properties from First- and S.pdf:application/pdf},
}

@article{zhou_optimization_2019,
	title = {Optimization of {Molecules} via {Deep} {Reinforcement} {Learning}},
	volume = {9},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-019-47148-x},
	doi = {10.1038/s41598-019-47148-x},
	language = {en},
	number = {1},
	urldate = {2021-08-22},
	journal = {Scientific Reports},
	author = {Zhou, Zhenpeng and Kearnes, Steven and Li, Li and Zare, Richard N. and Riley, Patrick},
	month = dec,
	year = {2019},
	pages = {10752},
	file = {Zhou et al. - 2019 - Optimization of Molecules via Deep Reinforcement L.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\VFCEWSQ6\\Zhou et al. - 2019 - Optimization of Molecules via Deep Reinforcement L.pdf:application/pdf},
}

@article{zhang_generic_2015,
	title = {Generic mathematical programming formulation and solution for computer-aided molecular design},
	volume = {78},
	issn = {00981354},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135415001234},
	doi = {10.1016/j.compchemeng.2015.04.022},
	language = {en},
	urldate = {2021-08-22},
	journal = {Computers \& Chemical Engineering},
	author = {Zhang, Lei and Cignitti, Stefano and Gani, Rafiqul},
	month = jul,
	year = {2015},
	pages = {79--84},
	file = {Zhang et al. - 2015 - Generic mathematical programming formulation and s.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\RS9SQWZX\\Zhang et al. - 2015 - Generic mathematical programming formulation and s.pdf:application/pdf},
}

@article{marrero_group-contribution_2001,
	title = {Group-contribution based estimation of pure component properties},
	volume = {183-184},
	issn = {03783812},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378381201004319},
	doi = {10.1016/S0378-3812(01)00431-9},
	abstract = {A new method for the estimation of properties of pure organic compounds is presented. Estimation is performed at three levels. The primary level uses contributions from simple groups that allow describing a wide variety of organic compounds, while the higher levels involve polyfunctional and structural groups that provide more information about molecular fragments whose description through ﬁrst-order groups is not possible. The presented method allows estimations of the following properties: normal boiling point, critical temperature, critical pressure, critical volume, standard enthalpy of formation, standard enthalpy of vaporization, standard Gibbs energy, normal melting point and standard enthalpy of fusion. The group-contribution tables have been developed from regression using a data set of more than 2000 compounds ranging from C = 3–60, including large and complex polycyclic compounds. Compared to the currently used group-contribution methods, the new method makes signiﬁcant improvements both in accuracy and applicability. © 2001 Elsevier Science B.V. All rights reserved.},
	language = {en},
	urldate = {2021-08-22},
	journal = {Fluid Phase Equilibria},
	author = {Marrero, Jorge and Gani, Rafiqul},
	month = jul,
	year = {2001},
	pages = {183--208},
	file = {Marrero and Gani - 2001 - Group-contribution based estimation of pure compon.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\TUAUZ6CS\\Marrero and Gani - 2001 - Group-contribution based estimation of pure compon.pdf:application/pdf},
}

@article{austin_computer-aided_2016,
	title = {Computer-aided molecular design: {An} introduction and review of tools, applications, and solution techniques},
	volume = {116},
	issn = {02638762},
	shorttitle = {Computer-aided molecular design},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0263876216303446},
	doi = {10.1016/j.cherd.2016.10.014},
	abstract = {This article provides an introduction to and review of the ﬁeld of computer-aided molecular design (CAMD). It is intended to be approachable for the absolute beginner as well as useful to the seasoned CAMD practitioner. We begin by discussing various quantitative structure-property relationships (QSPRs) which have been demonstrated to work well with CAMD problems. The methods discussed in this article are (1) group contribution methods, (2) topological indices, and (3) signature descriptors. Next, we present general optimization formulations for various forms of the CAMD problem. Common design constraints are discussed and structural feasibility constraints are provided for the three types of QSPRs addressed. We then detail useful techniques for approaching CAMD optimization problems, including decomposition methods, heuristic approaches, and mathematical programming strategies. Finally, we discuss many applications that have been addressed using CAMD.},
	language = {en},
	urldate = {2021-08-22},
	journal = {Chemical Engineering Research and Design},
	author = {Austin, Nick D. and Sahinidis, Nikolaos V. and Trahan, Daniel W.},
	month = dec,
	year = {2016},
	pages = {2--26},
	file = {Austin et al. - 2016 - Computer-aided molecular design An introduction a.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\PGHV7UDF\\Austin et al. - 2016 - Computer-aided molecular design An introduction a.pdf:application/pdf},
}

@incollection{wobcke_limitations_2008,
	address = {Berlin, Heidelberg},
	title = {On the {Limitations} of {Scalarisation} for {Multi}-objective {Reinforcement} {Learning} of {Pareto} {Fronts}},
	volume = {5360},
	isbn = {978-3-540-89377-6 978-3-540-89378-3},
	url = {http://link.springer.com/10.1007/978-3-540-89378-3_37},
	abstract = {Multiobjective reinforcement learning (MORL) extends RL to problems with multip le conflicting objectives. This paper argues for designing MORL systems to produce a set of solutions approximating the Pareto front, and shows that the common MORL technique of scalarisation has fundamental limitations when used to find Pareto-optimal policies. The work is supported by the presentation of three new MORL benchmarks with known Pareto fronts.},
	language = {en},
	urldate = {2021-08-22},
	booktitle = {{AI} 2008: {Advances} in {Artificial} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vamplew, Peter and Yearwood, John and Dazeley, Richard and Berry, Adam},
	editor = {Wobcke, Wayne and Zhang, Mengjie},
	year = {2008},
	doi = {10.1007/978-3-540-89378-3_37},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {372--378},
	file = {Vamplew et al. - 2008 - On the Limitations of Scalarisation for Multi-obje.PDF:C\:\\Users\\Bryan\\Zotero\\storage\\XKQJNKRI\\Vamplew et al. - 2008 - On the Limitations of Scalarisation for Multi-obje.PDF:application/pdf},
}

@article{lin_pareto_nodate,
	title = {Pareto {Multi}-{Task} {Learning}},
	abstract = {Multi-task learning is a powerful method for solving multiple correlated tasks simultaneously. However, it is often impossible to ﬁnd one single solution to optimize all the tasks, since different tasks might conﬂict with each other. Recently, a novel method is proposed to ﬁnd one single Pareto optimal solution with good trade-off among different tasks by casting multi-task learning as multiobjective optimization. In this paper, we generalize this idea and propose a novel Pareto multi-task learning algorithm (Pareto MTL) to ﬁnd a set of well-distributed Pareto solutions which can represent different trade-offs among different tasks. The proposed algorithm ﬁrst formulates a multi-task learning problem as a multiobjective optimization problem, and then decomposes the multiobjective optimization problem into a set of constrained subproblems with different trade-off preferences. By solving these subproblems in parallel, Pareto MTL can ﬁnd a set of well-representative Pareto optimal solutions with different trade-off among all tasks. Practitioners can easily select their preferred solution from these Pareto solutions, or use different trade-off solutions for different situations. Experimental results conﬁrm that the proposed algorithm can generate well-representative solutions and outperform some state-of-the-art algorithms on many multi-task learning applications.},
	language = {en},
	author = {Lin, Xi and Zhen, Hui-Ling and Li, Zhenhua and Zhang, Qing-Fu and Kwong, Sam},
	pages = {11},
	file = {Lin et al. - Pareto Multi-Task Learning.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\BHUXJSZV\\Lin et al. - Pareto Multi-Task Learning.pdf:application/pdf},
}

@article{moaert_multi-objective_nodate,
	title = {Multi-{Objective} {Reinforcement} {Learning} using {Sets} of {Pareto} {Dominating} {Policies}},
	abstract = {Many real-world problems involve the optimization of multiple, possibly conﬂicting objectives. Multi-objective reinforcement learning (MORL) is a generalization of standard reinforcement learning where the scalar reward signal is extended to multiple feedback signals, in essence, one for each objective. MORL is the process of learning policies that optimize multiple criteria simultaneously. In this paper, we present a novel temporal diﬀerence learning algorithm that integrates the Pareto dominance relation into a reinforcement learning approach. This algorithm is a multi-policy algorithm that learns a set of Pareto dominating policies in a single run. We name this algorithm Pareto Q-learning and it is applicable in episodic environments with deterministic as well as stochastic transition functions. A crucial aspect of Pareto Q-learning is the updating mechanism that bootstraps sets of Q-vectors. One of our main contributions in this paper is a mechanism that separates the expected immediate reward vector from the set of expected future discounted reward vectors. This decomposition allows us to update the sets and to exploit the learned policies consistently throughout the state space. To balance exploration and exploitation during learning, we also propose three set evaluation mechanisms. These three mechanisms evaluate the sets of vectors to accommodate for standard action selection strategies, such as -greedy. More precisely, these mechanisms use multi-objective evaluation principles such as the hypervolume measure, the cardinality indicator and the Pareto dominance relation to select the most promising actions. We experimentally validate the algorithm on multiple environments with two and three objectives and we demonstrate that Pareto Q-learning outperforms current state-of-the-art MORL algorithms with respect to the hypervolume of the obtained policies. We note that (1) Pareto Q-learning is able to learn the entire Pareto front under the usual assumption that each state-action pair is suﬃciently sampled, while (2) not being biased by the shape of the Pareto front. Furthermore, (3) the set evaluation mechanisms provide indicative measures for local action selection and (4) the learned policies can be retrieved throughout the state and action space.},
	language = {en},
	author = {Moﬀaert, Kristof Van and Nowe, Ann},
	pages = {30},
	file = {Moﬀaert and Nowe - Multi-Objective Reinforcement Learning using Sets .pdf:C\:\\Users\\Bryan\\Zotero\\storage\\CMVZBZI4\\Moﬀaert and Nowe - Multi-Objective Reinforcement Learning using Sets .pdf:application/pdf},
}

@inproceedings{barrett_learning_2008,
	address = {Helsinki, Finland},
	title = {Learning all optimal policies with multiple criteria},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390162},
	doi = {10.1145/1390156.1390162},
	abstract = {We describe an algorithm for learning in the presence of multiple criteria. Our technique generalizes previous approaches in that it can learn optimal policies for all linear preference assignments over the multiple reward criteria at once. The algorithm can be viewed as an extension to standard reinforcement learning for MDPs where instead of repeatedly backing up maximal expected rewards, we back up the set of expected rewards that are maximal for some set of linear preferences (given by a weight vector, −→w ). We present the algorithm along with a proof of correctness showing that our solution gives the optimal policy for any linear preference function. The solution reduces to the standard value iteration algorithm for a speciﬁc weight vector, −→w .},
	language = {en},
	urldate = {2021-08-22},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Barrett, Leon and Narayanan, Srini},
	year = {2008},
	pages = {41--47},
	file = {Barrett and Narayanan - 2008 - Learning all optimal policies with multiple criter.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\7FC4JPP6\\Barrett and Narayanan - 2008 - Learning all optimal policies with multiple criter.pdf:application/pdf},
}

@article{zhou_provable_2021,
	title = {Provable {Multi}-{Objective} {Reinforcement} {Learning} with {Generative} {Models}},
	url = {http://arxiv.org/abs/2011.10134},
	abstract = {Multi-objective reinforcement learning (MORL) is an extension of ordinary, single-objective reinforcement learning (RL) that is applicable to many real-world tasks where multiple objectives exist without known relative costs. We study the problem of single policy MORL, which learns an optimal policy given the preference of objectives. Existing methods require strong assumptions such as exact knowledge of the multi-objective Markov decision process, and are analyzed in the limit of inﬁnite data and time. We propose a new algorithm called model-based envelop value iteration (EVI), which generalizes the enveloped multi-objective Qlearning algorithm in Yang et al. [21]. Our method can learn a near-optimal value function with polynomial sample complexity and linear convergence speed. To the best of our knowledge, this is the ﬁrst ﬁnite-sample analysis of MORL algorithms.},
	language = {en},
	urldate = {2021-08-22},
	journal = {arXiv:2011.10134 [cs, math]},
	author = {Zhou, Dongruo and Chen, Jiahao and Gu, Quanquan},
	month = jan,
	year = {2021},
	note = {arXiv: 2011.10134},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, 68T05, 68T20, I.2.6, I.2.8, Mathematics - Optimization and Control, IMPORTANT - PROCESS CONTROL},
}

@article{you_graph_2019,
	title = {Graph {Convolutional} {Policy} {Network} for {Goal}-{Directed} {Molecular} {Graph} {Generation}},
	url = {http://arxiv.org/abs/1806.02473},
	abstract = {Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to ﬁnd molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goaldirected graph generation through reinforcement learning. The model is trained to optimize domain-speciﬁc rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-speciﬁc rules. Experimental results show that GCPN can achieve 61\% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184\% improvement on the constrained property optimization task.},
	language = {en},
	urldate = {2021-08-22},
	journal = {arXiv:1806.02473 [cs, stat]},
	author = {You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
	month = feb,
	year = {2019},
	note = {arXiv: 1806.02473},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {You et al. - 2019 - Graph Convolutional Policy Network for Goal-Direct.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\QBI5C9A6\\You et al. - 2019 - Graph Convolutional Policy Network for Goal-Direct.pdf:application/pdf},
}

@article{efroni_exploration-exploitation_2020,
	title = {Exploration-{Exploitation} in {Constrained} {MDPs}},
	url = {http://arxiv.org/abs/2003.02189},
	abstract = {In many sequential decision-making problems, the goal is to optimize a utility function while satisfying a set of constraints on diﬀerent utilities. This learning problem is formalized through Constrained Markov Decision Processes (CMDPs). In this paper, we investigate the explorationexploitation dilemma in CMDPs. While learning in an unknown CMDP, an agent should trade-oﬀ exploration to discover new information about the MDP, and exploitation of the current knowledge to maximize the reward while satisfying the constraints. While the agent will eventually learn a good or optimal policy, we do not want the agent to violate the constraints too often during the learning process. In this work, we analyze two approaches for learning in CMDPs. The ﬁrst approach leverages the linear formulation of CMDP to perform optimistic planning at each episode. The second approach leverages the dual formulation (or saddle-point formulation) of CMDP to perform incremental, optimistic updates of the primal and dual variables. We show that both achieves sublinear regret w.r.t. the main utility while having a sublinear regret on the constraint violations. That being said, we highlight a crucial diﬀerence between the two approaches; the linear programming approach results in stronger guarantees than in the dual formulation based approach.},
	language = {en},
	urldate = {2021-08-22},
	journal = {arXiv:2003.02189 [cs, stat]},
	author = {Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.02189},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Efroni et al. - 2020 - Exploration-Exploitation in Constrained MDPs.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\7C8TCJQW\\Efroni et al. - 2020 - Exploration-Exploitation in Constrained MDPs.pdf:application/pdf},
}

@article{wang_multi-objective_nodate,
	title = {Multi-objective {Monte}-{Carlo} {Tree} {Search}},
	abstract = {Concerned with multi-objective reinforcement learning (MORL), this paper presents MOMCTS, an extension of Monte-Carlo Tree Search to multi-objective sequential decision making. The known multi-objective indicator referred to as hyper-volume indicator is used to deﬁne an action selection criterion, replacing the UCB criterion in order to deal with multi-dimensional rewards. MO-MCTS is ﬁrstly compared with an existing MORL algorithm on the artiﬁcial Deep Sea Treasure problem. Then a scalability study of MOMCTS is made on the NP-hard problem of grid scheduling, showing that the performance of MO-MCTS matches the non RL-based state of the art albeit with a higher computational cost.},
	language = {en},
	author = {Wang, Weijia and Sebag, Michele},
	pages = {16},
	file = {Wang and Sebag - Multi-objective Monte-Carlo Tree Search.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\K8RDS3VE\\Wang and Sebag - Multi-objective Monte-Carlo Tree Search.pdf:application/pdf},
}

@article{hukkerikar_group-contribution_2012,
	title = {Group-contribution+ ({GC}+) based estimation of properties of pure components: {Improved} property estimation and uncertainty analysis},
	volume = {321},
	issn = {03783812},
	shorttitle = {Group-contribution+ ({GC}+) based estimation of properties of pure components},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378381212000817},
	doi = {10.1016/j.fluid.2012.02.010},
	language = {en},
	urldate = {2021-08-22},
	journal = {Fluid Phase Equilibria},
	author = {Hukkerikar, Amol Shivajirao and Sarup, Bent and Ten Kate, Antoon and Abildskov, Jens and Sin, Gürkan and Gani, Rafiqul},
	month = may,
	year = {2012},
	pages = {25--43},
	file = {Hukkerikar et al. - 2012 - Group-contribution+ (GC+) based estimation of prop.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\9ZUNZKUE\\Hukkerikar et al. - 2012 - Group-contribution+ (GC+) based estimation of prop.pdf:application/pdf},
}

@misc{rocca_understanding_2021,
	title = {Understanding {Variational} {Autoencoders} ({VAEs})},
	url = {https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73},
	abstract = {Building, step by step, the reasoning that leads to VAEs.},
	language = {en},
	urldate = {2021-08-23},
	journal = {Medium},
	author = {Rocca, Joseph},
	month = mar,
	year = {2021},
	file = {Snapshot:C\:\\Users\\Bryan\\Zotero\\storage\\CWTCZC9L\\understanding-variational-autoencoders-vaes-f70510919f73.html:text/html},
}

@article{mazyavkina_reinforcement_2021,
	title = {Reinforcement learning for combinatorial optimization: {A} survey},
	volume = {134},
	issn = {03050548},
	shorttitle = {Reinforcement learning for combinatorial optimization},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054821001660},
	doi = {10.1016/j.cor.2021.105400},
	abstract = {Many traditional algorithms for solving combinatorial optimization problems involve using hand-crafted heuristics that sequentially construct a solution. Such heuristics are designed by domain experts and may often be suboptimal due to the hard nature of the problems. Reinforcement learning (RL) proposes a good alternative to automate the search of these heuristics by training an agent in a supervised or self-supervised manner. In this survey, we explore the recent advancements of applying RL frameworks to hard combinatorial problems. Our survey provides the necessary background for operations research and machine learning communities and showcases the works that are moving the field forward. We juxtapose recently proposed RL methods, laying out the timeline of the improvements for each problem, as well as we make a comparison with traditional algorithms, indicating that RL models can become a promising direction for solving combinatorial problems.},
	language = {en},
	urldate = {2021-08-24},
	journal = {Computers \& Operations Research},
	author = {Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and Burnaev, Evgeny},
	month = oct,
	year = {2021},
	pages = {105400},
	file = {Mazyavkina et al. - 2021 - Reinforcement learning for combinatorial optimizat.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\33QN8HFL\\Mazyavkina et al. - 2021 - Reinforcement learning for combinatorial optimizat.pdf:application/pdf},
}

@article{li_distance_nodate,
	title = {Distance {Encoding}: {Design} {Provably} {More} {Powerful} {Neural} {Networks} for {Graph} {Representation} {Learning}},
	abstract = {Learning representations of sets of nodes in a graph is crucial for applications ranging from node-role discovery to link prediction and molecule classiﬁcation. Graph Neural Networks (GNNs) have achieved great success in graph representation learning. However, expressive power of GNNs is limited by the 1-Weisfeiler-Lehman (WL) test and thus GNNs generate identical representations for graph substructures that may in fact be very different. More powerful GNNs, proposed recently by mimicking higher-order-WL tests, only focus on representing entire graphs and they are computationally inefﬁcient as they cannot utilize sparsity of the underlying graph. Here we propose and mathematically analyze a general class of structurerelated features, termed Distance Encoding (DE). DE assists GNNs in representing any set of nodes, while providing strictly more expressive power than the 1-WL test. DE captures the distance between the node set whose representation is to be learned and each node in the graph. To capture the distance DE can apply various graph-distance measures such as shortest path distance or generalized PageRank scores. We propose two ways for GNNs to use DEs (1) as extra node features, and (2) as controllers of message aggregation in GNNs. Both approaches can utilize the sparse structure of the underlying graph, which leads to computational efﬁciency and scalability. We also prove that DE can distinguish node sets embedded in almost all regular graphs where traditional GNNs always fail. We evaluate DE on three tasks over six real networks: structural role prediction, link prediction, and triangle prediction. Results show that our models outperform GNNs without DE by up-to 15\% in accuracy and AUROC. Furthermore, our models also signiﬁcantly outperform other state-of-the-art methods especially designed for the above tasks.},
	language = {en},
	author = {Li, Pan and Wang, Yanbang and Wang, Hongwei and Leskovec, Jure},
	pages = {29},
	file = {Li et al. - Distance Encoding Design Provably More Powerful N.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\Q9TD4QHH\\Li et al. - Distance Encoding Design Provably More Powerful N.pdf:application/pdf},
}

@article{coley_machine_2018,
	title = {Machine {Learning} in {Computer}-{Aided} {Synthesis} {Planning}},
	volume = {51},
	issn = {0001-4842, 1520-4898},
	url = {https://pubs.acs.org/doi/10.1021/acs.accounts.8b00087},
	doi = {10.1021/acs.accounts.8b00087},
	language = {en},
	number = {5},
	urldate = {2021-08-29},
	journal = {Accounts of Chemical Research},
	author = {Coley, Connor W. and Green, William H. and Jensen, Klavs F.},
	month = may,
	year = {2018},
	pages = {1281--1289},
	file = {Coley et al. - 2018 - Machine Learning in Computer-Aided Synthesis Plann.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\9MNXB4T8\\Coley et al. - 2018 - Machine Learning in Computer-Aided Synthesis Plann.pdf:application/pdf},
}

@article{agarwal_pc-pg_2020,
	title = {{PC}-{PG}: {Policy} {Cover} {Directed} {Exploration} for {Provable} {Policy} {Gradient} {Learning}},
	shorttitle = {{PC}-{PG}},
	url = {http://arxiv.org/abs/2007.08459},
	abstract = {Direct policy gradient methods for reinforcement learning are a successful approach for a variety of reasons: they are model free, they directly optimize the performance metric of interest, and they allow for richly parameterized policies. Their primary drawback is that, by being local in nature, they fail to adequately explore the environment. In contrast, while model-based approaches and Q-learning directly handle exploration through the use of optimism, their ability to handle model misspeciﬁcation and function approximation is far less evident. This work introduces the the Policy Cover-Policy Gradient (PC-PG) algorithm, which provably balances the exploration vs. exploitation tradeoff using an ensemble of learned policies (the policy cover). PC-PG enjoys polynomial sample complexity and run time for both tabular MDPs and, more generally, linear MDPs in an inﬁnite dimensional RKHS. Furthermore, PC-PG also has strong guarantees under model misspeciﬁcation that go beyond the standard worst case ∞ assumptions; this includes approximation guarantees for state aggregation under an average case error assumption, along with guarantees under a more general assumption where the approximation error under distribution shift is controlled. We complement the theory with empirical evaluation across a variety of domains in both reward-free and reward-driven settings.},
	language = {en},
	urldate = {2021-09-01},
	journal = {arXiv:2007.08459 [cs, stat]},
	author = {Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
	month = aug,
	year = {2020},
	note = {arXiv: 2007.08459},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, In Progress},
	file = {Agarwal et al. - 2020 - PC-PG Policy Cover Directed Exploration for Prova.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\B7XCPMDT\\Agarwal et al. - 2020 - PC-PG Policy Cover Directed Exploration for Prova.pdf:application/pdf},
}

@article{duvenaud_convolutional_2015,
	title = {Convolutional {Networks} on {Graphs} for {Learning} {Molecular} {Fingerprints}},
	url = {http://arxiv.org/abs/1509.09292},
	abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular ﬁngerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	language = {en},
	urldate = {2021-09-03},
	journal = {arXiv:1509.09292 [cs, stat]},
	author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P.},
	month = nov,
	year = {2015},
	note = {arXiv: 1509.09292},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\27RPM5YT\\Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf:application/pdf},
}

@article{brunke_safe_2021,
	title = {Safe {Learning} in {Robotics}: {From} {Learning}-{Based} {Control} to {Safe} {Reinforcement} {Learning}},
	shorttitle = {Safe {Learning} in {Robotics}},
	url = {http://arxiv.org/abs/2108.06266},
	abstract = {The last half-decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. Our review includes: learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As dataand learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximity to humans. We highlight some of the open challenges that will drive the ﬁeld of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches.},
	language = {en},
	urldate = {2021-09-04},
	journal = {arXiv:2108.06266 [cs, eess]},
	author = {Brunke, Lukas and Greeff, Melissa and Hall, Adam W. and Yuan, Zhaocong and Zhou, Siqi and Panerati, Jacopo and Schoellig, Angela P.},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.06266},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Computer Science - Robotics},
	file = {Brunke et al. - 2021 - Safe Learning in Robotics From Learning-Based Con.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\GQHEPHBM\\Brunke et al. - 2021 - Safe Learning in Robotics From Learning-Based Con.pdf:application/pdf},
}

@inproceedings{dornheim_multiobjective_2018,
	address = {Timisoara},
	title = {Multiobjective {Reinforcement} {Learning} for {Reconfigurable} {Adaptive} {Optimal} {Control} of {Manufacturing} {Processes}},
	isbn = {978-1-5386-5925-0},
	url = {https://ieeexplore.ieee.org/document/8583854/},
	doi = {10.1109/ISETC.2018.8583854},
	abstract = {In industrial applications of adaptive optimal control often multiple contrary objectives have to be considered. The relative importance (weights) of the objectives are often not known during the design of the control and can change with changing production conditions and requirements. In this work a novel model-free multiobjective reinforcement learning approach for adaptive optimal control of manufacturing processes is proposed. The approach enables sample-efﬁcient learning in sequences of control conﬁgurations, given by particular objective weights.},
	language = {en},
	urldate = {2021-09-08},
	booktitle = {2018 {International} {Symposium} on {Electronics} and {Telecommunications} ({ISETC})},
	publisher = {IEEE},
	author = {Dornheim, Johannes and Link, Norbert},
	month = nov,
	year = {2018},
	pages = {1--5},
	file = {Dornheim and Link - 2018 - Multiobjective Reinforcement Learning for Reconfig.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\9H8IIIBK\\Dornheim and Link - 2018 - Multiobjective Reinforcement Learning for Reconfig.pdf:application/pdf},
}

@article{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efﬁciently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node’s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classiﬁcation benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	language = {en},
	urldate = {2021-09-09},
	journal = {arXiv:1706.02216 [cs, stat]},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv: 1706.02216},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	file = {Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\ASHKD83V\\Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:application/pdf},
}

@article{krenn_self-referencing_2020,
	title = {Self-{Referencing} {Embedded} {Strings} ({SELFIES}): {A} 100\% robust molecular string representation},
	volume = {1},
	issn = {2632-2153},
	shorttitle = {Self-{Referencing} {Embedded} {Strings} ({SELFIES})},
	url = {http://arxiv.org/abs/1905.13741},
	doi = {10.1088/2632-2153/aba947},
	abstract = {The discovery of novel materials and functional molecules can help to solve some of society's most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering -- generally denoted as inverse design -- was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100{\textbackslash}\% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model's internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.},
	language = {en},
	number = {4},
	urldate = {2021-09-09},
	journal = {Machine Learning: Science and Technology},
	author = {Krenn, Mario and Häse, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Alán},
	month = nov,
	year = {2020},
	note = {arXiv: 1905.13741},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Chemical Physics, Quantum Physics},
	pages = {045024},
	file = {Krenn et al. - 2020 - Self-Referencing Embedded Strings (SELFIES) A 100.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\J3E9KKLV\\Krenn et al. - 2020 - Self-Referencing Embedded Strings (SELFIES) A 100.pdf:application/pdf},
}

@article{liu_optcamd_2019,
	title = {{OptCAMD}: {An} optimization-based framework and tool for molecular and mixture product design},
	volume = {124},
	issn = {00981354},
	shorttitle = {{OptCAMD}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135418310925},
	doi = {10.1016/j.compchemeng.2019.01.006},
	language = {en},
	urldate = {2021-09-10},
	journal = {Computers \& Chemical Engineering},
	author = {Liu, Qilei and Zhang, Lei and Liu, Linlin and Du, Jian and Tula, Anjan Kumar and Eden, Mario and Gani, Rafiqul},
	month = may,
	year = {2019},
	pages = {285--301},
	file = {Liu et al. - 2019 - OptCAMD An optimization-based framework and tool .pdf:C\:\\Users\\Bryan\\Zotero\\storage\\85T6R4IN\\Liu et al. - 2019 - OptCAMD An optimization-based framework and tool .pdf:application/pdf},
}

@article{seider_product_nodate,
	title = {Product and {Process} {Design} {Principles}},
	language = {en},
	author = {Seider, Warren D},
	pages = {775},
	file = {Seider - Product and Process Design Principles.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\USWGLESI\\Seider - Product and Process Design Principles.pdf:application/pdf},
}

@article{chai_grand_2020,
	title = {A grand product design model for crystallization solvent design},
	volume = {135},
	issn = {00981354},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0098135419311329},
	doi = {10.1016/j.compchemeng.2020.106764},
	language = {en},
	urldate = {2021-09-10},
	journal = {Computers \& Chemical Engineering},
	author = {Chai, Shiyang and Liu, Qilei and Liang, Xinyuan and Guo, Yansuo and Zhang, Song and Xu, Chengqiu and Du, Jian and Yuan, Zhihong and Zhang, Lei and Gani, Rafiqul},
	month = apr,
	year = {2020},
	pages = {106764},
	file = {Chai et al. - 2020 - A grand product design model for crystallization s.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\GVXHHSHI\\Chai et al. - 2020 - A grand product design model for crystallization s.pdf:application/pdf},
}

@article{hu_learning_2020,
	title = {Learning to {Utilize} {Shaping} {Rewards}: {A} {New} {Approach} of {Reward} {Shaping}},
	shorttitle = {Learning to {Utilize} {Shaping} {Rewards}},
	url = {http://arxiv.org/abs/2011.02669},
	abstract = {Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimization problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneﬁcial shaping rewards, and meanwhile ignore unbeneﬁcial shaping rewards or even transform them into beneﬁcial ones.},
	language = {en},
	urldate = {2021-10-10},
	journal = {arXiv:2011.02669 [cs]},
	author = {Hu, Yujing and Wang, Weixun and Jia, Hangtian and Wang, Yixiang and Chen, Yingfeng and Hao, Jianye and Wu, Feng and Fan, Changjie},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.02669},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Hu et al. - 2020 - Learning to Utilize Shaping Rewards A New Approac.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\WRSHM3X7\\Hu et al. - 2020 - Learning to Utilize Shaping Rewards A New Approac.pdf:application/pdf},
}

@article{korshunova_bag_nodate,
	title = {A bag of tricks for automated de novo design of molecules with the desired properties: application to {EGFR} inhibitor discovery},
	abstract = {Deep generative neural networks have been used increasingly in computational chemistry for de novo design of molecules with desired properties. Many deep learning approaches employ reinforcement learning for optimizing the target properties of the generated molecules. However, the success of this approach is often hampered by the problem of sparse rewards as the majority of the generated molecules are expectedly predicted as inactives. We propose several technical innovations to address this problem and improve the balance between exploration and exploitation modes in reinforcement learning. In a proof-of-concept study, we demonstrate the application of the deep generative recurrent neural network enhanced by several novel technical tricks to designing experimentally validated potent inhibitors of the epidermal growth factor (EGFR). The proposed technical solutions are expected to substantially improve the success rate of finding novel bioactive compounds for specific biological targets using generative and reinforcement learning approaches.},
	language = {en},
	author = {Korshunova, Maria and Huang, Niles and Capuzzi, Stephen and Radchenko, Dmytro S and Savych, Olena and Moroz, Yuriy S and Wells, Carrow I and Willson, Timothy M and Tropsha, Alexander and Isayev, Olexandr},
	pages = {19},
	file = {Korshunova et al. - A bag of tricks for automated de novo design of mo.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\T5WULPZA\\Korshunova et al. - A bag of tricks for automated de novo design of mo.pdf:application/pdf},
}

@article{jembre_evaluation_2021,
	title = {Evaluation of {Reinforcement} and {Deep} {Learning} {Algorithms} in {Controlling} {Unmanned} {Aerial} {Vehicles}},
	volume = {11},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/16/7240},
	doi = {10.3390/app11167240},
	abstract = {Unmanned Aerial Vehicles (UAVs) are abundantly becoming a part of society, which is a trend that is expected to grow even further. The quadrotor is one of the drone technologies that is applicable in many sectors and in both military and civilian activities, with some applications requiring autonomous ﬂight. However, stability, path planning, and control remain signiﬁcant challenges in autonomous quadrotor ﬂights. Traditional control algorithms, such as proportionalintegral-derivative (PID), have deﬁciencies, especially in tuning. Recently, machine learning has received great attention in ﬂying UAVs to desired positions autonomously. In this work, we conﬁgure the quadrotor to ﬂy autonomously by using agents (the machine learning schemes being used to ﬂy the quadrotor autonomously) to learn about the virtual physical environment. The quadrotor will ﬂy from an initial to a desired position. When the agent brings the quadrotor closer to the desired position, it is rewarded; otherwise, it is punished. Two reinforcement learning models, Q-learning and SARSA, and a deep learning deep Q-network network are used as agents. The simulation is conducted by integrating the robot operating system (ROS) and Gazebo, which allowed for the implementation of the learning algorithms and the physical environment, respectively. The result has shown that the Deep Q-network network with Adadelta optimizer is the best setting to ﬂy the quadrotor from the initial to desired position.},
	language = {en},
	number = {16},
	urldate = {2021-10-11},
	journal = {Applied Sciences},
	author = {Jembre, Yalew Zelalem and Nugroho, Yuniarto Wimbo and Khan, Muhammad Toaha Raza and Attique, Muhammad and Paul, Rajib and Shah, Syed Hassan Ahmed and Kim, Beomjoon},
	month = aug,
	year = {2021},
	pages = {7240},
	file = {Jembre et al. - 2021 - Evaluation of Reinforcement and Deep Learning Algo.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\FSJR4KC8\\Jembre et al. - 2021 - Evaluation of Reinforcement and Deep Learning Algo.pdf:application/pdf},
}

@article{zitnik_modeling_2018,
	title = {Modeling polypharmacy side effects with graph convolutional networks},
	volume = {34},
	issn = {1367-4803, 1460-2059},
	url = {https://academic.oup.com/bioinformatics/article/34/13/i457/5045770},
	doi = {10.1093/bioinformatics/bty294},
	abstract = {Motivation: The use of drug combinations, termed polypharmacy, is common to treat patients with complex diseases or co-existing conditions. However, a major consequence of polypharmacy is a much higher risk of adverse side effects for the patient. Polypharmacy side effects emerge because of drug–drug interactions, in which activity of one drug may change, favorably or unfavorably, if taken with another drug. The knowledge of drug interactions is often limited because these complex relationships are rare, and are usually not observed in relatively small clinical testing. Discovering polypharmacy side effects thus remains an important challenge with signiﬁcant implications for patient mortality and morbidity.},
	language = {en},
	number = {13},
	urldate = {2021-10-11},
	journal = {Bioinformatics},
	author = {Zitnik, Marinka and Agrawal, Monica and Leskovec, Jure},
	month = jul,
	year = {2018},
	pages = {i457--i466},
	file = {Zitnik et al. - 2018 - Modeling polypharmacy side effects with graph conv.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\8XYNJEZY\\Zitnik et al. - 2018 - Modeling polypharmacy side effects with graph conv.pdf:application/pdf},
}

@article{muller_flexible_2019,
	title = {Flexible heuristic algorithm for automatic molecule fragmentation: application to the {UNIFAC} group contribution model},
	volume = {11},
	issn = {1758-2946},
	shorttitle = {Flexible heuristic algorithm for automatic molecule fragmentation},
	url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0382-3},
	doi = {10.1186/s13321-019-0382-3},
	abstract = {A priori calculation of thermophysical properties and predictive thermodynamic models can be very helpful for developing new industrial processes. Group contribution methods link the target property to contributions based on chemical groups or other molecular subunits of a given molecule. However, the fragmentation of the molecule into its subunits is usually done manually impeding the fast testing and development of new group contribution methods based on large databases of molecules. The aim of this work is to develop strategies to overcome the challenges that arise when attempting to fragment molecules automatically while keeping the definition of the groups as simple as possible. Furthermore, these strategies are implemented in two fragmentation algorithms. The first algorithm finds only one solution while the second algorithm finds all possible fragmentations. Both algorithms are tested to fragment a database of 20,000+ molecules for use with the group contribution model Universal Quasichemical Functional Group Activity Coefficients (UNIFAC). Comparison of the results with a reference database shows that both algorithms are capable of successfully fragmenting all the molecules automatically. Furthermore, when applying them on a larger database it is shown, that the newly developed algorithms are capable of fragmenting structures previously thought not possible to fragment.},
	language = {en},
	number = {1},
	urldate = {2021-11-06},
	journal = {Journal of Cheminformatics},
	author = {Müller, Simon},
	month = dec,
	year = {2019},
	pages = {57},
	file = {Müller - 2019 - Flexible heuristic algorithm for automatic molecul.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\GQQ2UEMR\\Müller - 2019 - Flexible heuristic algorithm for automatic molecul.pdf:application/pdf},
}

@article{ioffe_batch_nodate,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classiﬁcation: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	language = {en},
	author = {Ioffe, Sergey and Szegedy, Christian},
	pages = {9},
	file = {Ioffe and Szegedy - Batch Normalization Accelerating Deep Network Tra.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\NZN2UPWD\\Ioffe and Szegedy - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}

@article{rahimi_random_nodate,
	title = {Random {Features} for {Large}-{Scale} {Kernel} {Machines}},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shiftinvariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
	language = {en},
	author = {Rahimi, Ali and Recht, Benjamin},
	pages = {8},
	file = {Rahimi and Recht - Random Features for Large-Scale Kernel Machines.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\PU95KSAR\\Rahimi and Recht - Random Features for Large-Scale Kernel Machines.pdf:application/pdf},
}

@inproceedings{weinberger_feature_2009,
	address = {Montreal, Quebec, Canada},
	title = {Feature hashing for large scale multitask learning},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553516},
	doi = {10.1145/1553374.1553516},
	abstract = {Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case — multitask learning with hundreds of thousands of tasks.},
	language = {en},
	urldate = {2021-11-21},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Weinberger, Kilian and Dasgupta, Anirban and Langford, John and Smola, Alex and Attenberg, Josh},
	year = {2009},
	pages = {1--8},
	file = {Weinberger et al. - 2009 - Feature hashing for large scale multitask learning.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\RIM9GTKJ\\Weinberger et al. - 2009 - Feature hashing for large scale multitask learning.pdf:application/pdf},
}

@inproceedings{ma_identifying_2009,
	address = {Montreal, Quebec, Canada},
	title = {Identifying suspicious {URLs}: an application of large-scale online learning},
	isbn = {978-1-60558-516-1},
	shorttitle = {Identifying suspicious {URLs}},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553462},
	doi = {10.1145/1553374.1553462},
	abstract = {This paper explores online learning approaches for detecting malicious Web sites (those involved in criminal scams) using lexical and host-based features of the associated URLs. We show that this application is particularly appropriate for online algorithms as the size of the training data is larger than can be efﬁciently processed in batch and because the distribution of features that typify malicious URLs is changing continuously. Using a real-time system we developed for gathering URL features, combined with a real-time source of labeled URLs from a large Web mail provider, we demonstrate that recentlydeveloped online algorithms can be as accurate as batch techniques, achieving classiﬁcation accuracies up to 99\% over a balanced data set.},
	language = {en},
	urldate = {2021-11-21},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	year = {2009},
	pages = {1--8},
	file = {Ma et al. - 2009 - Identifying suspicious URLs an application of lar.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\Q6BE75WC\\Ma et al. - 2009 - Identifying suspicious URLs an application of lar.pdf:application/pdf},
}

@article{johnson_accelerating_nodate,
	title = {Accelerating {Stochastic} {Gradient} {Descent} using {Predictive} {Variance} {Reduction}},
	abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is signiﬁcantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
	language = {en},
	author = {Johnson, Rie and Zhang, Tong},
	pages = {9},
	file = {Johnson and Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\4DNSRKI2\\Johnson and Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf:application/pdf},
}

@article{bergstra_random_nodate,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua},
	pages = {25},
	file = {Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\MQCY2FKX\\Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf:application/pdf},
}

@article{li_system_nodate,
	title = {A {System} for {Massively} {Parallel} {Hyperparameter} {Tuning}},
	abstract = {Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by ﬁrst introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI’s end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.},
	language = {en},
	author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Ben-Tzur, Jonathan and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
	pages = {17},
	file = {Li et al. - A System for Massively Parallel Hyperparameter Tun.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\J5WVVGDY\\Li et al. - A System for Massively Parallel Hyperparameter Tun.pdf:application/pdf},
}

@article{wilson_marginal_nodate,
	title = {The {Marginal} {Value} of {Adaptive} {Gradient} {Methods} in {Machine} {Learning}},
	abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often ﬁnd drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classiﬁcation problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several stateof-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often signiﬁcantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
	language = {en},
	author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
	pages = {11},
	file = {Wilson et al. - The Marginal Value of Adaptive Gradient Methods in.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\396W24KT\\Wilson et al. - The Marginal Value of Adaptive Gradient Methods in.pdf:application/pdf},
}

@article{chu_map-reduce_nodate,
	title = {Map-{Reduce} for {Machine} {Learning} on {Multicore}},
	abstract = {We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and uniﬁed way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Speciﬁcally, we show that algorithms that ﬁt the Statistical Query model [15] can be written in a certain “summation form,” which allows them to be easily parallelized on multicore computers. We adapt Google’s map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.},
	language = {en},
	author = {Chu, Cheng-tao and Kim, Sang K and Lin, Yi-an and Yu, Yuanyuan and Bradski, Gary and Olukotun, Kunle and Ng, Andrew Y},
	pages = {8},
	file = {Chu et al. - Map-Reduce for Machine Learning on Multicore.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\6Y6IKXJJ\\Chu et al. - Map-Reduce for Machine Learning on Multicore.pdf:application/pdf},
}

@article{recht_hogwild_nodate,
	title = {Hogwild: {A} {Lock}-{Free} {Approach} to {Parallelizing} {Stochastic} {Gradient} {Descent}},
	abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve stateof-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performancedestroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other’s work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
	language = {en},
	author = {Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
	pages = {9},
	file = {Recht et al. - Hogwild A Lock-Free Approach to Parallelizing Sto.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\8HDEN93X\\Recht et al. - Hogwild A Lock-Free Approach to Parallelizing Sto.pdf:application/pdf},
}

@article{dean_large_nodate,
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	abstract = {Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.},
	language = {en},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and Ng, Andrew Y},
	pages = {9},
	file = {Dean et al. - Large Scale Distributed Deep Networks.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\A95YRZHH\\Dean et al. - Large Scale Distributed Deep Networks.pdf:application/pdf},
}

@article{bonawitz_towards_nodate,
	title = {Towards {Federated} {Learning} at {Scale}: {System} {Design}},
	abstract = {Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.},
	language = {en},
	journal = {System Design},
	author = {Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloé and Konecný, Jakub and Mazzocchi, Stefano and McMahan, H Brendan and Overveldt, Timon Van and Petrou, David and Ramage, Daniel and Roselander, Jason},
	pages = {15},
	file = {Bonawitz et al. - Towards Federated Learning at Scale System Design.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\A4ZKXSKY\\Bonawitz et al. - Towards Federated Learning at Scale System Design.pdf:application/pdf},
}

@article{gupta_deep_nodate,
	title = {Deep {Learning} with {Limited} {Numerical} {Precision}},
	abstract = {Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of lowprecision ﬁxed-point computations, we observe the rounding scheme to play a crucial role in determining the network’s behavior during training. Our results show that deep networks can be trained using only 16-bit wide ﬁxed-point number representation when using stochastic rounding, and incur little to no degradation in the classiﬁcation accuracy. We also demonstrate an energy-efﬁcient hardware accelerator that implements low-precision ﬁxed-point arithmetic with stochastic rounding.},
	language = {en},
	author = {Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
	pages = {10},
	file = {Gupta et al. - Deep Learning with Limited Numerical Precision.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\P2D7W46N\\Gupta et al. - Deep Learning with Limited Numerical Precision.pdf:application/pdf},
}

@article{courbariaux_binaryconnect_nodate,
	title = {{BinaryConnect}: {Training} {Deep} {Neural} {Networks} with binary weights during propagations},
	abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great beneﬁts to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
	language = {en},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	pages = {9},
	file = {Courbariaux et al. - BinaryConnect Training Deep Neural Networks with .pdf:C\:\\Users\\Bryan\\Zotero\\storage\\8JBFN9JH\\Courbariaux et al. - BinaryConnect Training Deep Neural Networks with .pdf:application/pdf},
}

@article{blalock_what_nodate,
	title = {What is the {State} of {Neural} {Network} {Pruning}?},
	abstract = {Neural network pruning—the task of reducing the size of a network by removing parameters—has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent ﬁndings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest ﬁnding is that the community suffers from a lack of standardized benchmarks and metrics. This deﬁciency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the ﬁeld has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
	language = {en},
	author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
	pages = {18},
	file = {Blalock et al. - What is the State of Neural Network Pruning.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\IAP26PA2\\Blalock et al. - What is the State of Neural Network Pruning.pdf:application/pdf},
}

@article{smilkov_tensorflowjs_nodate,
	title = {{TensorFlow}.js: {Machine} {Learning} for the {Web} and {Beyond}},
	abstract = {TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.},
	language = {en},
	author = {Smilkov, Daniel and Thorat, Nikhil and Assogba, Yannick and Yuan, Ann and Kreeger, Nick and Yu, Ping and Zhang, Kangyi and Cai, Shanqing and Nielsen, Eric and Soergel, David and Bileschi, Stan and Terry, Michael and Nicholson, Charles and Gupta, Sandeep N and Sirajuddin, Sarah and Sculley, D and Monga, Rajat and Corrado, Greg and Viégas, Fernanda B and Wattenberg, Martin},
	pages = {13},
	file = {Smilkov et al. - TensorFlow.js Machine Learning for the Web and Be.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\BF7J24SJ\\Smilkov et al. - TensorFlow.js Machine Learning for the Web and Be.pdf:application/pdf},
}

@article{paszke_pytorch_nodate,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientiﬁc computing libraries, while remaining efﬁcient and supporting hardware accelerators such as GPUs.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	pages = {12},
	file = {Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\LVSEYYPU\\Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf},
}

@article{jouppi_-datacenter_nodate,
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {UnitTM}},
	language = {en},
	author = {Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Kaplan, Alexander and Khaitan, Harshit and Killebrew, Daniel and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
	pages = {17},
	file = {Jouppi et al. - In-Datacenter Performance Analysis of a Tensor Pro.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\UBQVR72L\\Jouppi et al. - In-Datacenter Performance Analysis of a Tensor Pro.pdf:application/pdf},
}

@article{gajane_variational_2019,
	title = {Variational {Regret} {Bounds} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1905.05857},
	abstract = {We consider undiscounted reinforcement learning in Markov decision processes (MDPs) where both the reward functions and the state-transition probabilities may vary (gradually or abruptly) over time. For this problem setting, we propose an algorithm and provide performance guarantees for the regret evaluated against the optimal non-stationary policy. The upper bound on the regret is given in terms of the total variation in the MDP. This is the ﬁrst variational regret bound for the general reinforcement learning setting.},
	language = {en},
	urldate = {2021-12-08},
	journal = {arXiv:1905.05857 [cs, stat]},
	author = {Gajane, Pratik and Ortner, Ronald and Auer, Peter},
	month = sep,
	year = {2019},
	note = {arXiv: 1905.05857},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Gajane et al. - 2019 - Variational Regret Bounds for Reinforcement Learni.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\NTPWG82G\\Gajane et al. - 2019 - Variational Regret Bounds for Reinforcement Learni.pdf:application/pdf},
}

@incollection{richmond_preface_2020,
	title = {Preface},
	isbn = {978-3-11-068657-9},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110686579-201/html},
	language = {en},
	urldate = {2022-03-03},
	booktitle = {General {Topology}},
	publisher = {De Gruyter},
	collaborator = {Richmond, Tom},
	month = jul,
	year = {2020},
	doi = {10.1515/9783110686579-201},
	pages = {V--VIII},
	file = {2020 - Preface.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\GXLHYDZ2\\2020 - Preface.pdf:application/pdf},
}

@article{chanachichalermwong_krafft_2019,
	title = {Krafft {Point} {Prediction} of {Anionic} {Surfactants} {Using} {Group} {Contribution} {Method}: {First}‐{Order} and {Higher}‐{Order} {Groups}},
	volume = {22},
	issn = {1097-3958, 1558-9293},
	shorttitle = {Krafft {Point} {Prediction} of {Anionic} {Surfactants} {Using} {Group} {Contribution} {Method}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jsde.12290},
	doi = {10.1002/jsde.12290},
	abstract = {Krafft point is one of the key properties of anionic surfactants to indicate their solubility in an aqueous phase. Below the Krafft point, the surfactant is a useless solid. We have developed a prediction model for the Krafft point based on the Marrero and Gani Group Contribution (GC) concept. Literature data were analyzed and a new third-order level was proposed to improve the model accuracy. The Krafft points of 53 anionic surfactants were collected including alkyl sulfonate, alkyl sulfate, alkyl benzene sulfonate, alkyl ethoxy sulfate, alkyl ester sulfonate, alkyl ester sulfate, alkyl disulfate, alkyldiphenylether disulfonates, alkyl diester sulfonate, alkyl naphthalene sulfonate, and ﬂuorocarbon surfactants. The coefﬁcients of the fractional groups in a surfactant molecule reveal the dependence of the Krafft point on the internal structure. Compared to the more complicated models such as those based on advanced statistical concept, the GC model for the Krafft point developed in this work gives a higher accuracy with a simpler calculation.},
	language = {en},
	number = {4},
	urldate = {2022-03-20},
	journal = {Journal of Surfactants and Detergents},
	author = {Chanachichalermwong, Woramet and Charoensaeng, Ampira and Suriyapraphadilok, Uthaiporn},
	month = jul,
	year = {2019},
	pages = {907--919},
	file = {Chanachichalermwong et al. - 2019 - Krafft Point Prediction of Anionic Surfactants Usi.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\GJAX2VEC\\Chanachichalermwong et al. - 2019 - Krafft Point Prediction of Anionic Surfactants Usi.pdf:application/pdf},
}

@article{lan_sub-gmn_2021,
	title = {Sub-{GMN}: {The} {Subgraph} {Matching} {Network} {Model}},
	shorttitle = {Sub-{GMN}},
	url = {http://arxiv.org/abs/2104.00186},
	abstract = {As one of the most fundamental tasks in graph theory, subgraph matching is a crucial task in many ﬁelds, ranging from information retrieval, computer vision, biology, chemistry and natural language processing. Yet subgraph matching problem remains to be an NP-complete problem. This study proposes an end-to-end learning-based approximate method for subgraph matching task, called subgraph matching network (Sub-GMN). The proposed Sub-GMN ﬁrstly uses graph representation learning to map nodes to node-level embedding. It then combines metric learning and attention mechanisms to model the relationship between matched nodes in the data graph and query graph. To test the performance of the proposed method, we applied our method on two databases. We used two existing methods, GNN and FGNN as baseline for comparison. Our experiment shows that, on dataset 1, on average the accuracy of Sub-GMN are 12.21\% and 3.2\% higher than that of GNN and FGNN respectively. On average running time Sub-GMN runs 20-40 times faster than FGNN. In addition, the average F1-score of Sub-GMN on all experiments with dataset 2 reached 0.95, which demonstrates that Sub-GMN outputs more correct node-to-node matches.},
	language = {en},
	urldate = {2022-03-20},
	journal = {arXiv:2104.00186 [cs]},
	author = {Lan, Zixun and Yu, Limin and Yuan, Linglong and Wu, Zili and Niu, Qiang and Ma, Fei},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.00186},
	keywords = {Computer Science - Machine Learning, Computer Science - Discrete Mathematics},
	file = {Lan et al. - 2021 - Sub-GMN The Subgraph Matching Network Model.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\8YGUDKTE\\Lan et al. - 2021 - Sub-GMN The Subgraph Matching Network Model.pdf:application/pdf},
}

@article{yang_hit_nodate,
	title = {Hit and {Lead} {Discovery} with {Explorative} {RL} and {Fragment}-based {Molecule} {Generation}},
	abstract = {Recently, utilizing reinforcement learning (RL) to generate molecules with desired properties has been highlighted as a promising strategy for drug design. A molecular docking program – a physical simulation that estimates protein-small molecule binding afﬁnity – can be an ideal reward scoring function for RL, as it is a straightforward proxy of the therapeutic potential. Still, two imminent challenges exist for this task. First, the models often fail to generate chemically realistic and pharmacochemically acceptable molecules. Second, the docking score optimization is a difﬁcult exploration problem that involves many local optima and less smooth surfaces with respect to molecular structure. To tackle these challenges, we propose a novel RL framework that generates pharmacochemically acceptable molecules with large docking scores. Our method – Fragment-based generative RL with Explorative Experience replay for Drug design (FREED) – constrains the generated molecules to a realistic and qualiﬁed chemical space and effectively explores the space to ﬁnd drugs by coupling our fragment-based generation method and a novel error-prioritized experience replay (PER). We also show that our model performs well on both de novo and scaffold-based schemes. Our model produces molecules of higher quality compared to existing methods while achieving state-of-the-art performance on two of three targets in terms of the docking scores of the generated molecules. We further show with ablation studies that our method, predictive error-PER (FREED(PE)), signiﬁcantly improves the model performance.},
	language = {en},
	author = {Yang, Soojung and Hwang, Doyeong and Lee, Seul and Ryu, Seongok and Hwang, Sung Ju},
	pages = {13},
	file = {Yang et al. - Hit and Lead Discovery with Explorative RL and Fra.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\R4QUBLYH\\Yang et al. - Hit and Lead Discovery with Explorative RL and Fra.pdf:application/pdf},
}

@article{fey_deep_2020,
	title = {Deep {Graph} {Matching} {Consensus}},
	url = {http://arxiv.org/abs/2001.09621},
	abstract = {This work presents a two-stage neural architecture for learning and reﬁning structural correspondences between graphs. First, we use localized node embeddings computed by a graph neural network to obtain an initial ranking of soft correspondences between nodes. Secondly, we employ synchronous message passing networks to iteratively re-rank the soft correspondences to reach a matching consensus in local neighborhoods between graphs. We show, theoretically and empirically, that our message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, which is then used to guide the iterative re-ranking process. Our purely local and sparsity-aware architecture scales well to large, real-world inputs while still being able to recover global correspondences consistently. We demonstrate the practical effectiveness of our method on real-world tasks from the ﬁelds of computer vision and entity alignment between knowledge graphs, on which we improve upon the current state-of-theart. Our source code is available under https://github.com/rusty1s/ deep-graph-matching-consensus.},
	language = {en},
	urldate = {2022-04-23},
	journal = {arXiv:2001.09621 [cs, stat]},
	author = {Fey, Matthias and Lenssen, Jan E. and Morris, Christopher and Masci, Jonathan and Kriege, Nils M.},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.09621},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fey et al. - 2020 - Deep Graph Matching Consensus.pdf:C\:\\Users\\Bryan\\Zotero\\storage\\9HP94L2V\\Fey et al. - 2020 - Deep Graph Matching Consensus.pdf:application/pdf},
}
